{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as fn\n",
    "import pyspark.sql.types as tp\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "            .master(\"local[2]\")\n",
    "            .config(\"spark.driver.bindAddress\", \"localhost\")\n",
    "            .config(\"spark.driver.port\", \"8080\")\n",
    "            .config(\"spark.driver.memory\", \"2g\")\n",
    "            .config(\"spark.driver.host\", \"localhost\")\n",
    "            .config(\"spark.executor.memory\", \"3g\" )\n",
    "            .config(\"spark.executor.cores\", \"5\" )\n",
    "            .config(\"spark.dynamicAllocation.enabled\", \"true\" )\n",
    "            .config(\"spark.default.parallelism\", \"2\" )\n",
    "            .config(\"spark.shuffle.io.retryWait\", \"2000ms\" )\n",
    "            .config(\"spark.shuffle.io.maxRetries\", \"2\" )\n",
    "            .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDfWithSuffixedColumns(df, suffix):\n",
    "    return df.select([fn.col(c).alias(f\"{c}{suffix}\") for c in df.columns])\n",
    "\n",
    "def mergeDatasetOnKey(suffix, df1, df2): \n",
    "    df_new = getDfWithSuffixedColumns(df2, suffix=suffix)\n",
    "    return df1.join(df_new, df1[f\"key{suffix}\"]==df_new[f\"pkey{suffix}\"], \"inner\")\n",
    "\n",
    "def dropColumns(training, db):\n",
    "    '''\n",
    "    Returns the training_dataset and cleaned_data_dataset with dropped (aka unnecessary columns)\n",
    "    '''\n",
    "    dbColsToDrop = [\n",
    "    \"_c0\",\n",
    "    \"paddress\",\n",
    "    \"ppublisher\",\n",
    "    \"pseries\",\n",
    "    \"pbooktitlefull_id\",\n",
    "    \"pjournalfull_id\",\n",
    "    \"peditor\",\n",
    "    \"pbooktitle_id\",\n",
    "    \"partition\"\n",
    "    ]\n",
    "    trainColsToDrop = [\"id\", \"partition\"]\n",
    "    return (training.drop(*trainColsToDrop), db.drop(*dbColsToDrop))\n",
    "\n",
    "\n",
    "def jaccard_similarity(a, b):\n",
    "    # convert to set\n",
    "    a = set(a)\n",
    "    b = set(b)\n",
    "    # calucate jaccard similarity\n",
    "    j = float(len(a.intersection(b))) / len(a.union(b))\n",
    "    return j\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(key1='conf/semweb/PaolucciKPS02', key2='conf/woa/SacileMPB00', label='False', pyear2='2000.0', pid2='778642', pkey2='conf/woa/SacileMPB00', ptype_id2='4', pjournal_id2='0', clean_author2='Roberto Sacile|Ernesto Montaldo|Massimo Paolucci 0002|Antonio Boccalatte', clean_title2='Intelligent agents applied to manufacturing: the MAKE-IT approach.', pyear1='2002.0', pid1='683591', pkey1='conf/semweb/PaolucciKPS02', ptype_id1='1', pjournal_id1='0', clean_author1='Massimo Paolucci|Takahiro Kawamura|Terry R. Payne|Katia P. Sycara', clean_title1='Semantic Matching of Web Services Capabilities.'),\n",
       " Row(key1='conf/cases/AkgulLM01', key2='conf/date/AkgulM01', label='True', pyear2='2001.0', pid2='166440', pkey2='conf/date/AkgulM01', ptype_id2='4', pjournal_id2='0', clean_author2='Bilge Saglam Akgul|Vincent John Mooney III', clean_title2='System-on-a-chip processor synchronization support in hardware.', pyear1='2001.0', pid1='91031', pkey1='conf/cases/AkgulLM01', ptype_id1='4', pjournal_id1='0', clean_author1='Bilge Saglam Akgul|Jaehwan Lee|Vincent John Mooney III', clean_title1='A system-on-a-chip lock cache with task preemption support.')]"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.read.csv(\"data/db/db.csv\", sep=\"!\", header=True)\n",
    "train_df = spark.read.csv(\"data/train.csv\", header=True)\n",
    "\n",
    "(train_df, df) = dropColumns(train_df, df)\n",
    "train_df =  mergeDatasetOnKey(\"1\", mergeDatasetOnKey(\"2\", train_df, df), df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(key1='conf/semweb/PaolucciKPS02', key2='conf/woa/SacileMPB00', label='False', pyear2='2000.0', pid2='778642', pkey2='conf/woa/SacileMPB00', ptype_id2='4', pjournal_id2='0', clean_author2='Roberto Sacile|Ernesto Montaldo|Massimo Paolucci 0002|Antonio Boccalatte', clean_title2='Intelligent agents applied to manufacturing: the MAKE-IT approach.', pyear1='2002.0', pid1='683591', pkey1='conf/semweb/PaolucciKPS02', ptype_id1='1', pjournal_id1='0', clean_author1='Massimo Paolucci|Takahiro Kawamura|Terry R. Payne|Katia P. Sycara', clean_title1='Semantic Matching of Web Services Capabilities.', jaccard_author=0.5625, jaccard_title=0.5625, jaccard_key=0.6190476190476191, diff_year=-2.0),\n",
       " Row(key1='conf/cases/AkgulLM01', key2='conf/date/AkgulM01', label='True', pyear2='2001.0', pid2='166440', pkey2='conf/date/AkgulM01', ptype_id2='4', pjournal_id2='0', clean_author2='Bilge Saglam Akgul|Vincent John Mooney III', clean_title2='System-on-a-chip processor synchronization support in hardware.', pyear1='2001.0', pid1='91031', pkey1='conf/cases/AkgulLM01', ptype_id1='4', pjournal_id1='0', clean_author1='Bilge Saglam Akgul|Jaehwan Lee|Vincent John Mooney III', clean_title1='A system-on-a-chip lock cache with task preemption support.', jaccard_author=0.92, jaccard_title=0.75, jaccard_key=0.7894736842105263, diff_year=0.0)]"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jaccard_udf = fn.udf(jaccard_similarity, fn.StringType())\n",
    "\n",
    "def addColumns(training_df):\n",
    "    temp_df = training_df\n",
    "\n",
    "    temp_df = temp_df.withColumn(\"jaccard_author\", jaccard_udf(*[\"clean_author1\", \"clean_author2\"]).cast(\"Double\"))\n",
    "    temp_df = temp_df.withColumn(\"jaccard_title\", jaccard_udf(*[\"clean_title1\", \"clean_title2\"]).cast(\"Double\"))\n",
    "    temp_df = temp_df.withColumn(\"jaccard_key\", jaccard_udf(*[\"key1\", \"key2\"]).cast(\"Double\"))\n",
    "    temp_df = temp_df.withColumn(\"diff_year\", temp_df.pyear2 - temp_df.pyear1)\n",
    "    return temp_df\n",
    "\n",
    "train_df = addColumns(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(key1='conf/semweb/PaolucciKPS02', key2='conf/woa/SacileMPB00', label=0, pyear2='2000.0', pid2='778642', pkey2='conf/woa/SacileMPB00', ptype_id2='4', pjournal_id2='0', clean_author2='Roberto Sacile|Ernesto Montaldo|Massimo Paolucci 0002|Antonio Boccalatte', clean_title2='Intelligent agents applied to manufacturing: the MAKE-IT approach.', pyear1='2002.0', pid1='683591', pkey1='conf/semweb/PaolucciKPS02', ptype_id1='1', pjournal_id1='0', clean_author1='Massimo Paolucci|Takahiro Kawamura|Terry R. Payne|Katia P. Sycara', clean_title1='Semantic Matching of Web Services Capabilities.', jaccard_author=0.5625, jaccard_title=0.5625, jaccard_key=0.6190476190476191, diff_year=-2.0, features=DenseVector([0.5625, 0.5625, 0.619, -2.0])),\n",
       " Row(key1='conf/cases/AkgulLM01', key2='conf/date/AkgulM01', label=1, pyear2='2001.0', pid2='166440', pkey2='conf/date/AkgulM01', ptype_id2='4', pjournal_id2='0', clean_author2='Bilge Saglam Akgul|Vincent John Mooney III', clean_title2='System-on-a-chip processor synchronization support in hardware.', pyear1='2001.0', pid1='91031', pkey1='conf/cases/AkgulLM01', ptype_id1='4', pjournal_id1='0', clean_author1='Bilge Saglam Akgul|Jaehwan Lee|Vincent John Mooney III', clean_title1='A system-on-a-chip lock cache with task preemption support.', jaccard_author=0.92, jaccard_title=0.75, jaccard_key=0.7894736842105263, diff_year=0.0, features=DenseVector([0.92, 0.75, 0.7895, 0.0]))]"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "required_features = [\n",
    "                    'jaccard_author',\n",
    "                    'jaccard_title',\n",
    "                    'jaccard_key',\n",
    "                    'diff_year'\n",
    "                   ]\n",
    "\n",
    "assembler = VectorAssembler(inputCols=required_features, outputCol='features')\n",
    "\n",
    "transformed_data = assembler.transform(train_df)\n",
    "# transformed_data.show()\n",
    "transformed_data = transformed_data.withColumn(\"label\", transformed_data.label.cast('boolean').cast('integer'))\n",
    "\n",
    "transformed_data.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "[train, test] = transformed_data.randomSplit([0.9, 0.1], seed=1000)\n",
    "\n",
    "test_with_label = test\n",
    "test_without_label = test.drop(\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "\n",
    "rf = RandomForestClassifier(labelCol='label', \n",
    "                            featuresCol='features',\n",
    "                            maxDepth=8)\n",
    "model = rf.fit(train)\n",
    "rf_predictions = model.transform(test_with_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "\n",
    "reg = LogisticRegression(labelCol='label')\n",
    "reg_model = reg.fit(train)\n",
    "reg_predictions = reg_model.transform(test_with_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest classifier Accuracy: 0.7843137254901961\n",
      "Logistic Regression Accuracy: 0.7855392156862745\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "multi_evaluator = MulticlassClassificationEvaluator(labelCol = 'label', metricName = 'accuracy')\n",
    "print('Random Forest classifier Accuracy:', multi_evaluator.evaluate(rf_predictions))\n",
    "print('Logistic Regression Accuracy:', multi_evaluator.evaluate(reg_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ae8b999abbdff59552af0f1df588bea1b451364c8ca3ce9ecd4beeea233850ce"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 ('bigd': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
